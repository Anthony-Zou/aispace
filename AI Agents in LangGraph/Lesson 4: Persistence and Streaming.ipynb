{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e73683f4",
   "metadata": {},
   "source": [
    "# Lesson 4: Persistence and Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850db66f",
   "metadata": {},
   "source": [
    "## What are Persistence and Streaming used for?\n",
    "\n",
    "### **Persistence (Checkpointing)**\n",
    "\n",
    "1. **Multi-turn conversations** - Agent remembers previous messages\n",
    "   - Example: \"What's the weather in SF?\" â†’ \"What about LA?\" â†’ \"Which is warmer?\" \n",
    "   - The agent remembers SF and LA context from earlier questions\n",
    "\n",
    "2. **Multiple users/sessions** - Different `thread_id` keeps conversations separate\n",
    "   - User A's chat history stays separate from User B's\n",
    "   - Essential for multi-user applications\n",
    "\n",
    "3. **Resume after interruption** - Save state, restart later\n",
    "   - Long-running tasks can be paused and continued\n",
    "   - Prevents loss of progress if system crashes\n",
    "\n",
    "4. **Human-in-the-loop** - Stop for approval, then continue\n",
    "   - Agent asks \"Should I delete this file?\" â†’ waits for human decision â†’ continues\n",
    "   - Critical for safety in automated systems\n",
    "\n",
    "### **Streaming**\n",
    "\n",
    "1. **Real-time UI updates** - Like ChatGPT's typing effect\n",
    "   - Show tokens as they generate instead of waiting for complete response\n",
    "   - Users see progress immediately\n",
    "\n",
    "2. **Progress monitoring** - See what agent is doing\n",
    "   - \"Searching web...\" â†’ \"Analyzing results...\" â†’ \"Generating answer...\"\n",
    "   - Transparency in agent workflow\n",
    "\n",
    "3. **Better UX** - Users don't stare at blank screen\n",
    "   - Especially important for slow LLM responses (30+ seconds)\n",
    "   - Reduces perceived wait time\n",
    "\n",
    "4. **Debug/logging** - Track agent workflow in real-time\n",
    "   - See each tool call, LLM invocation, and state update as it happens\n",
    "   - Easier troubleshooting during development\n",
    "\n",
    "### **Real-world Use Cases**\n",
    "\n",
    "- **Customer service chatbots** - Persistence for conversation context across messages\n",
    "- **Code assistants** - Streaming for real-time feedback during multi-step code generation\n",
    "- **Research agents** - Both: save progress while streaming results as they're found\n",
    "- **Data processing pipelines** - Checkpoint progress, stream status updates\n",
    "- **Interactive tutoring systems** - Remember student history, show explanations incrementally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e0ddbc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Libries\n",
    "from dotenv import load_dotenv\n",
    "_ = load_dotenv()\n",
    "load_dotenv('/home/jovyan/work/.env')  # Docker mounted directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94564f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated\n",
    "import operator\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b7f1aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_222/4289725543.py:1: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the `langchain-tavily package and should be used instead. To use it run `pip install -U `langchain-tavily` and import as `from `langchain_tavily import TavilySearch``.\n",
      "  tool = TavilySearchResults(max_results=2)\n"
     ]
    }
   ],
   "source": [
    "tool = TavilySearchResults(max_results=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95ce414a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], operator.add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dd40011",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, model, tools, checkpointer, system=\"\"):\n",
    "        self.system = system\n",
    "        graph = StateGraph(AgentState)\n",
    "        graph.add_node(\"llm\", self.call_openai)\n",
    "        graph.add_node(\"action\", self.take_action)\n",
    "        graph.add_conditional_edges(\"llm\", self.exists_action, {True: \"action\", False: END})\n",
    "        graph.add_edge(\"action\", \"llm\")\n",
    "        graph.set_entry_point(\"llm\")\n",
    "        self.graph = graph.compile(checkpointer=checkpointer)\n",
    "        self.tools = {t.name: t for t in tools}\n",
    "        self.model = model.bind_tools(tools)\n",
    "\n",
    "    def call_openai(self, state: AgentState):\n",
    "        messages = state['messages']\n",
    "        if self.system:\n",
    "            messages = [SystemMessage(content=self.system)] + messages\n",
    "        message = self.model.invoke(messages)\n",
    "        return {'messages': [message]}\n",
    "\n",
    "    def exists_action(self, state: AgentState):\n",
    "        result = state['messages'][-1]\n",
    "        return len(result.tool_calls) > 0\n",
    "\n",
    "    def take_action(self, state: AgentState):\n",
    "        tool_calls = state['messages'][-1].tool_calls\n",
    "        results = []\n",
    "        for t in tool_calls:\n",
    "            print(f\"Calling: {t}\")\n",
    "            result = self.tools[t['name']].invoke(t['args'])\n",
    "            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n",
    "        print(\"Back to the model!\")\n",
    "        return {'messages': results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d40a1fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "conn = sqlite3.connect(\":memory:\", check_same_thread=False)\n",
    "memory = SqliteSaver(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ac7e7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"You are a smart research assistant. Use the search engine to look up information. \\\n",
    "You are allowed to make multiple calls (either together or in sequence). \\\n",
    "Only look up information when you are sure of what you want. \\\n",
    "If you need to look up some information before asking a follow up question, you are allowed to do that!\n",
    "\"\"\"\n",
    "model = ChatOpenAI(model=\"gpt-4o\")\n",
    "abot = Agent(model, [tool], system=prompt, checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "105bdf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"What is the weather in sf?\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ecb07d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcdd6212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 151, 'total_tokens': 173, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_ad98c18a04', 'id': 'chatcmpl-DABMvpROIPGdv1S9VPtThew5gLFVU', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019c6adc-5e2d-7fc3-bf6e-315ebeac3139-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'current weather in San Francisco'}, 'id': 'call_vaTIun02txERBQJM6BA745GG', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 151, 'output_tokens': 22, 'total_tokens': 173, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
      "Calling: {'name': 'tavily_search_results_json', 'args': {'query': 'current weather in San Francisco'}, 'id': 'call_vaTIun02txERBQJM6BA745GG', 'type': 'tool_call'}\n",
      "Back to the model!\n",
      "[ToolMessage(content='[{\\'title\\': \\'Weather for San Francisco\\', \\'url\\': \\'https://www.peoplesweather.com/weather/San+Francisco/?date=2026-02-17\\', \\'content\\': \\'# Weather for San Francisco\\\\n\\\\n##### Tuesday 17 February 2026\\\\n\\\\n|  |  |  |\\\\n --- \\\\n|  | 9Â°CFeels like: 6Â°C | WSW / 23km/h  Moderate Breeze |\\\\n| Chance of Rain. Cool | | |\\\\n\\\\n|  |  |\\\\n --- |\\\\n| Pressure | 1007mb |\\\\n| Humidity | 82% |\\\\n| Rain | 60% |\\\\n| Cloud Cover | 100% |\\\\n| Dew Point | 6Â°C |\\\\n\\\\n|  |  |  |\\\\n --- \\\\n| This Afternoon | | |\\\\n|  | 9Â°C | WSW / 27km/h  Moderate Breeze |\\\\n| Chance of Rain. Cool | | |\\\\n\\\\n|  |  |  |\\\\n --- \\\\n| Tonight | | |\\\\n|  | 7Â°C | W / 23km/h  Moderate Breeze |\\\\n| Partly Cloudy. Cold | | |\\\\n\\\\n### 7 Day Forecast for San Francisco\\\\n\\\\n##### Weather\\\\n\\\\n##### Popular\\\\n\\\\n##### About us\\\\n\\\\n##### Newsletter\\\\n\\\\nÂ© 2007-2026 PeopleÂ°s Weather Pty. Ltd., All rights reserved.\\', \\'score\\': 0.9456058}, {\\'title\\': \\'San Francisco, CA Weather Conditions | Weather Underground\\', \\'url\\': \\'https://www.wunderground.com/weather/us/ca/san-francisco\\', \\'content\\': \"# San Francisco, CA Weather Conditionsstar\\\\\\\\_ratehome\\\\n\\\\nicon\\\\n\\\\nThank you for reporting this station. We will review the data in question.\\\\n\\\\nYou are about to report this weather station for bad data. Please select the information that is incorrect.\\\\n\\\\nSee more\\\\n\\\\n(Reset Map)\\\\n\\\\nNo PWS\\\\n\\\\nReset Map, or Add PWS.\\\\n\\\\naccess\\\\\\\\_time 1:07 AM PST on February 17, 2026 (GMT -8) | Updated 15 seconds ago\\\\n\\\\nicon\\\\n\\\\nCloudy\\\\n\\\\nTomorrow\\'s temperature is forecast to be NEARLY THE SAME as today.\\\\n\\\\nicon\\\\nicon\\\\nicon\\\\nicon\\\\nicon\\\\nicon\\\\nicon\\\\nicon\\\\nAccess Logo\\\\n\\\\nWe recognize our responsibility to use data and technology for good. We may use or share your data with our data vendors. Take control of your data.\\\\n\\\\nThe Weather Company Logo\\\\nThe Weather Channel Logo\\\\nWeather Underground Logo\\\\nStorm Radar Logo\\\\n\\\\nÂ© The Weather Company, LLC 2026\", \\'score\\': 0.853046}]', name='tavily_search_results_json', tool_call_id='call_vaTIun02txERBQJM6BA745GG')]\n",
      "[AIMessage(content='The current weather in San Francisco is as follows:\\n\\n- Temperature: 9Â°C, feels like 6Â°C\\n- Wind: WSW at 23 km/h (moderate breeze)\\n- Chance of Rain: 60% \\n- Cloud cover: 100%\\n- Humidity: 82%\\n- Pressure: 1007 mb\\n- Dew Point: 6Â°C\\n\\nThis afternoon, the temperature will remain around 9Â°C with wind from the WSW at 27 km/h, and there is still a chance of rain. Tonight, it will get colder with temperatures dropping to 7Â°C and a partly cloudy sky with wind coming from the west at 23 km/h.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 140, 'prompt_tokens': 758, 'total_tokens': 898, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_ad98c18a04', 'id': 'chatcmpl-DABN2EDuHTol7VBs6ZWCRoxlFJtYg', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019c6adc-7943-7463-aa86-62da642d484c-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 758, 'output_tokens': 140, 'total_tokens': 898, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n"
     ]
    }
   ],
   "source": [
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v['messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3e83742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 910, 'total_tokens': 932, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_64dfa806c7', 'id': 'chatcmpl-DABNKWGLCvmi8PQVqgRMrv4Y2qkii', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019c6adc-bd9c-7962-af7a-e2df8690432e-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'current weather in Los Angeles'}, 'id': 'call_UXaYcRLpc9PVvhLA2TUquNTC', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 910, 'output_tokens': 22, 'total_tokens': 932, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n",
      "Calling: {'name': 'tavily_search_results_json', 'args': {'query': 'current weather in Los Angeles'}, 'id': 'call_UXaYcRLpc9PVvhLA2TUquNTC', 'type': 'tool_call'}\n",
      "Back to the model!\n",
      "{'messages': [ToolMessage(content='[{\\'title\\': \\'Weather for Los Angeles\\', \\'url\\': \\'https://www.peoplesweather.com/weather/Los+Angeles/?date=2026-02-17\\', \\'content\\': \\'# Weather for Los Angeles\\\\n\\\\n##### Tuesday 17 February 2026\\\\n\\\\n|  |  |  |\\\\n --- \\\\n|  | 11Â°CFeels like: 7Â°C | SW / 43km/h  Strong Breeze |\\\\n| Sunny. Cool | | |\\\\n\\\\n|  |  |\\\\n --- |\\\\n| Pressure | 1009mb |\\\\n| Humidity | 32% |\\\\n| Rain | 0% |\\\\n| Cloud Cover | 11% |\\\\n| Dew Point | 1Â°C |\\\\n\\\\n|  |  |  |\\\\n --- \\\\n| This Afternoon | | |\\\\n|  | 11Â°C | SW / 52km/h  Near Gale |\\\\n| Partly Cloudy. Cool | | |\\\\n\\\\n|  |  |  |\\\\n --- \\\\n| Tonight | | |\\\\n|  | 7Â°C | SW / 31km/h  Fresh Breeze |\\\\n| Partly Cloudy. Cold | | |\\\\n\\\\n### 7 Day Forecast for Los Angeles\\\\n\\\\n##### Weather\\\\n\\\\n##### Popular\\\\n\\\\n##### About us\\\\n\\\\n##### Newsletter\\\\n\\\\nÂ© 2007-2026 PeopleÂ°s Weather Pty. Ltd., All rights reserved.\\', \\'score\\': 0.9988575}, {\\'title\\': \\'Los Angeles, CA Monthly Weather - AccuWeather\\', \\'url\\': \\'https://www.accuweather.com/en/us/los-angeles/90012/february-weather/347625\\', \\'content\\': \"# Los Angeles, CA\\\\n\\\\nLos Angeles\\\\n\\\\nCalifornia\\\\n\\\\n## Around the Globe\\\\n\\\\nAround the Globe\\\\n\\\\n### Hurricane Tracker\\\\n\\\\n### Severe Weather\\\\n\\\\n### Radar & Maps\\\\n\\\\n### News & Features\\\\n\\\\n### Astronomy\\\\n\\\\n### Business\\\\n\\\\n### Climate\\\\n\\\\n### Health\\\\n\\\\n### Recreation\\\\n\\\\n### Sports\\\\n\\\\n### Travel\\\\n\\\\n### Warnings\\\\n\\\\n### Data Suite\\\\n\\\\n### Forensics\\\\n\\\\n### Advertising\\\\n\\\\n### Superior Accuracyâ„¢\\\\n\\\\n### Video\\\\n\\\\n### Winter Center\\\\n\\\\n## Monthly\\\\n\\\\n## February\\\\n\\\\n## 2026\\\\n\\\\n## Daily\\\\n\\\\n## Temperature Graph\\\\n\\\\n## Further Ahead\\\\n\\\\nFurther Ahead\\\\n\\\\n### March 2026\\\\n\\\\n### April 2026\\\\n\\\\n### May 2026\\\\n\\\\n## Around the Globe\\\\n\\\\nAround the Globe\\\\n\\\\n### Hurricane Tracker\\\\n\\\\n### Severe Weather\\\\n\\\\n### Radar & Maps\\\\n\\\\n### News\\\\n\\\\n### Video\\\\n\\\\n### Winter Center\\\\n\\\\nTop Stories\\\\n\\\\nWinter Weather\\\\n\\\\nCalifornia bracing for flooding, pass-closing snow and severe storms\\\\n\\\\n48 minutes ago\\\\n\\\\nTravel [...] 48 minutes ago\\\\n\\\\nTravel\\\\n\\\\nItalyâ€™s famous \\'loversâ€™ arch\\' crashes into the sea on Valentineâ€™s Day\\\\n\\\\n16 hours ago\\\\n\\\\nWeather Forecasts\\\\n\\\\nRecord warmth to expand across central, eastern US this week\\\\n\\\\n48 minutes ago\\\\n\\\\nWeather News\\\\n\\\\nShipwreck missing since 1872 discovered at bottom of Lake Michigan\\\\n\\\\n15 hours ago\\\\n\\\\nWeather Forecasts\\\\n\\\\nSnow, ice, rain and severe weather coming to central, eastern U.S.\\\\n\\\\n48 minutes ago\\\\n\\\\nFeatured Stories\\\\n\\\\nRecreation\\\\n\\\\nPresidents Day marks first Free National Park day in 2026\\\\n\\\\n19 hours ago\\\\n\\\\nWeather News\\\\n\\\\nWhat\\'s behind South Carolinaâ€™s recent earthquakes\\\\n\\\\n16 hours ago\\\\n\\\\nWeather News\\\\n\\\\nGray wolf tracked in Los Angeles County for first time\\\\n\\\\n5 days ago\\\\n\\\\nAstronomy\\\\n\\\\nA \\'ring of fire\\' eclipse is coming Feb. 17\\\\n\\\\n20 hours ago\\\\n\\\\nWeather News [...] 20 hours ago\\\\n\\\\nWeather News\\\\n\\\\n99% of Florida is in drought with almost no rain falling in February\\\\n\\\\n3 days ago\\\\n\\\\n## Weather Near Los Angeles:\\\\n\\\\n...\\\\n\\\\n...\\\\n\\\\n...\", \\'score\\': 0.94853723}]', name='tavily_search_results_json', tool_call_id='call_UXaYcRLpc9PVvhLA2TUquNTC')]}\n",
      "{'messages': [AIMessage(content='The current weather in Los Angeles is as follows:\\n\\n- Temperature: 11Â°C, feels like 7Â°C\\n- Wind: SW at 43 km/h (strong breeze)\\n- Conditions: Sunny and cool\\n- Humidity: 32%\\n- Pressure: 1009 mb\\n- Rain: 0%\\n- Cloud Cover: 11%\\n- Dew Point: 1Â°C\\n\\nThis afternoon, the temperature will be around 11Â°C with the wind increasing to SW at 52 km/h (near gale). It will be partly cloudy and cool. Tonight, temperatures will drop to 7Â°C with SW winds at 31 km/h (fresh breeze), and it will be partly cloudy and cold.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 144, 'prompt_tokens': 1787, 'total_tokens': 1931, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_64dfa806c7', 'id': 'chatcmpl-DABNPmvScmMTubmzEr8R254rPT2Th', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019c6adc-d233-7a30-9597-a356a8d38f84-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 1787, 'output_tokens': 144, 'total_tokens': 1931, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n"
     ]
    }
   ],
   "source": [
    "messages = [HumanMessage(content=\"What about in la?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d34ac2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [AIMessage(content='Los Angeles is currently warmer than San Francisco. Los Angeles has a temperature of 11Â°C, while San Francisco has a temperature of 9Â°C.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 31, 'prompt_tokens': 1943, 'total_tokens': 1974, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 1920}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_64dfa806c7', 'id': 'chatcmpl-DABNiOm3mR7PCwYNMbsf6UvIQ5tvM', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019c6add-1a50-76d3-a277-97e45eae2541-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 1943, 'output_tokens': 31, 'total_tokens': 1974, 'input_token_details': {'audio': 0, 'cache_read': 1920}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n"
     ]
    }
   ],
   "source": [
    "messages = [HumanMessage(content=\"Which one is warmer?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7958772e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [AIMessage(content='Could you please clarify what you are comparing in terms of warmth? Are you asking about the weather in different locations, the temperature of substances, or something else? Let me know so I can assist you effectively!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 43, 'prompt_tokens': 149, 'total_tokens': 192, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_64dfa806c7', 'id': 'chatcmpl-DABNsQDj1Q8fv7vqksnIpmPZbY5kP', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019c6add-45e0-7f40-8d55-b913c4187cf2-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 149, 'output_tokens': 43, 'total_tokens': 192, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n"
     ]
    }
   ],
   "source": [
    "messages = [HumanMessage(content=\"Which one is warmer?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559d3e86",
   "metadata": {},
   "source": [
    "## Streaming tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e3ec35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q aiosqlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4ebb32df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver\n",
    "import aiosqlite\n",
    "\n",
    "# Create the async memory properly - must be awaited in async context\n",
    "async_conn = await aiosqlite.connect(\":memory:\")\n",
    "async_memory = AsyncSqliteSaver(async_conn)\n",
    "abot = Agent(model, [tool], system=prompt, checkpointer=async_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "262314b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling: {'name': 'tavily_search_results_json', 'args': {'query': 'current weather in San Francisco'}, 'id': 'call_GZSh16C5ppQireER38RPCpmH', 'type': 'tool_call'}\n",
      "Back to the model!\n",
      "The| current| weather| in| San| Francisco| is| cool| with| a| temperature| of| |9|Â°C|,| which| feels| like| |6|Â°C|.| The| wind| is| coming| from| the| west|-s|outh|west| at| a| speed| of| |23| km|/h|,| creating| a| moderate| breeze|.| There| is| a| |60|%| chance| of| rain| with| |100|%| cloud| cover|.| The| humidity| is| at| |82|%,| and| the| atmospheric| pressure| is| |100|7| mb|.| The| forecast| for| the| afternoon| remains| similar|,| and| for| tonight|,| the| temperature| is| expected| to| drop| to| |7|Â°C| with| partly| cloudy| conditions|.|"
     ]
    }
   ],
   "source": [
    "messages = [HumanMessage(content=\"What is the weather in SF?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"4\"}}\n",
    "async for event in abot.graph.astream_events({\"messages\": messages}, thread, version=\"v1\"):\n",
    "    kind = event[\"event\"]\n",
    "    if kind == \"on_chat_model_stream\":\n",
    "        content = event[\"data\"][\"chunk\"].content\n",
    "        if content:\n",
    "            # Empty content in the context of OpenAI means\n",
    "            # that the model is asking for a tool to be invoked.\n",
    "            # So we only print non-empty content\n",
    "            print(content, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a5ff9d",
   "metadata": {},
   "source": [
    "## Additional Streaming Example: Real-time Research Assistant\n",
    "\n",
    "This example shows how streaming provides transparency in a multi-step research workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a0f05dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” STARTING RESEARCH AGENT\n",
      "\n",
      "============================================================\n",
      "\n",
      "ğŸ¤” Agent is thinking...\n",
      "Calling: {'name': 'tavily_search_results_json', 'args': {'query': 'GDP of California 2023'}, 'id': 'call_6q4qx7OkL1vqbmFxlvUoZlE8', 'type': 'tool_call'}\n",
      "\n",
      "ğŸ”§ Calling tool: tavily_search_results_json\n",
      "   Input: {'query': 'GDP of California 2023'}\n",
      "Calling: {'name': 'tavily_search_results_json', 'args': {'query': 'GDP of Texas 2023'}, 'id': 'call_LXbNfitEinQ8by2I4xtFT1nK', 'type': 'tool_call'}âœ… Tool completed\n",
      "\n",
      "\n",
      "ğŸ”§ Calling tool: tavily_search_results_json\n",
      "   Input: {'query': 'GDP of Texas 2023'}\n",
      "Back to the model!âœ… Tool completed\n",
      "\n",
      "\n",
      "ğŸ¤” Agent is thinking...\n",
      "As of 2023, California's GDP was about $3.9 trillion, whereas Texas's GDP was significantly lower at approximately $2.769 trillion. Therefore, California has a higher GDP than Texas.\n",
      "============================================================\n",
      "âœ… RESEARCH COMPLETE\n"
     ]
    }
   ],
   "source": [
    "# Example: Track agent workflow with streaming\n",
    "# This shows what the agent is doing at each step\n",
    "\n",
    "messages = [HumanMessage(content=\"Compare the GDP of California and Texas. Which is higher?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"5\"}}\n",
    "\n",
    "print(\"ğŸ” STARTING RESEARCH AGENT\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "async for event in abot.graph.astream_events({\"messages\": messages}, thread, version=\"v1\"):\n",
    "    kind = event[\"event\"]\n",
    "    \n",
    "    # Show when agent starts thinking\n",
    "    if kind == \"on_chain_start\" and event.get(\"name\") == \"llm\":\n",
    "        print(\"\\nğŸ¤” Agent is thinking...\")\n",
    "    \n",
    "    # Show when agent calls a tool\n",
    "    if kind == \"on_tool_start\":\n",
    "        tool_name = event.get(\"name\", \"unknown\")\n",
    "        print(f\"\\nğŸ”§ Calling tool: {tool_name}\")\n",
    "        print(f\"   Input: {event['data'].get('input', {})}\")\n",
    "    \n",
    "    # Show when tool returns result\n",
    "    if kind == \"on_tool_end\":\n",
    "        print(f\"âœ… Tool completed\")\n",
    "    \n",
    "    # Stream the actual response content\n",
    "    if kind == \"on_chat_model_stream\":\n",
    "        content = event[\"data\"][\"chunk\"].content\n",
    "        if content:\n",
    "            print(content, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ… RESEARCH COMPLETE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab20ee45",
   "metadata": {},
   "source": [
    "**What this demonstrates:**\n",
    "- **Transparency**: User sees each step (thinking, tool calls, results)\n",
    "- **Progress feedback**: No blank screen - shows agent is working\n",
    "- **Real-time streaming**: Final answer appears token-by-token\n",
    "- **Better debugging**: Can see exactly where agent might get stuck\n",
    "\n",
    "**Output format:**\n",
    "```\n",
    "ğŸ” STARTING RESEARCH AGENT\n",
    "============================================================\n",
    "\n",
    "ğŸ¤” Agent is thinking...\n",
    "\n",
    "ğŸ”§ Calling tool: tavily_search_results_json\n",
    "   Input: {'query': 'California GDP 2024'}\n",
    "âœ… Tool completed\n",
    "\n",
    "ğŸ”§ Calling tool: tavily_search_results_json\n",
    "   Input: {'query': 'Texas GDP 2024'}\n",
    "âœ… Tool completed\n",
    "\n",
    "ğŸ¤” Agent is thinking...\n",
    "\n",
    "Based on the search results, California has a GDP of...\n",
    "[streaming tokens appear here in real-time]\n",
    "============================================================\n",
    "âœ… RESEARCH COMPLETE\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70942b8",
   "metadata": {},
   "source": [
    "## Comparison: Non-Streaming vs Streaming\n",
    "\n",
    "Let's see the difference more clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e384fd22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ NON-STREAMING EXAMPLE\n",
      "============================================================\n",
      "â³ Waiting... (user sees nothing)\n",
      "Calling: {'name': 'tavily_search_results_json', 'args': {'query': 'current date in Singapore'}, 'id': 'call_cBIDdWxH2ODa3IGPs5CWjmfH', 'type': 'tool_call'}\n",
      "Back to the model!\n",
      "\n",
      "ğŸ’¥ BOOM! Everything appears after 4.1s:\n",
      "Today's date in Singapore is February 17, 2026.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Use sync memory for non-streaming example\n",
    "sync_abot = Agent(model, [tool], system=prompt, checkpointer=memory)\n",
    "\n",
    "# NON-STREAMING: You wait, then see everything at once\n",
    "print(\"âŒ NON-STREAMING EXAMPLE\")\n",
    "print(\"=\" * 60)\n",
    "print(\"â³ Waiting... (user sees nothing)\")\n",
    "\n",
    "messages = [HumanMessage(content=\"What's the todays date in Singapore?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"6\"}}\n",
    "\n",
    "start = time.time()\n",
    "result = sync_abot.graph.invoke({\"messages\": messages}, thread)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"\\nğŸ’¥ BOOM! Everything appears after {elapsed:.1f}s:\")\n",
    "print(result['messages'][-1].content)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "56e230ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… STREAMING EXAMPLE\n",
      "============================================================\n",
      "ğŸ‘€ User sees everything as it happens:\n",
      "\n",
      "Calling: {'name': 'tavily_search_results_json', 'args': {'query': 'current date in Singapore'}, 'id': 'call_Ydte8v4VH2D48t7eLA9wmrgn', 'type': 'tool_call'}\n",
      "ğŸ”§ Tool: tavily_search_results_json\n",
      "Back to the model!\n",
      "The current date in Singapore is Tuesday, February 17, 2026.\n",
      "\n",
      "ğŸ’¡ Streamed 16 tokens over 5.8s\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# STREAMING: You see progress immediately\n",
    "print(\"\\nâœ… STREAMING EXAMPLE\")\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ‘€ User sees everything as it happens:\\n\")\n",
    "\n",
    "messages = [HumanMessage(content=\"What's the todays date in Singapore?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"7\"}}\n",
    "\n",
    "start = time.time()\n",
    "token_count = 0\n",
    "\n",
    "async for event in abot.graph.astream_events({\"messages\": messages}, thread, version=\"v1\"):\n",
    "    kind = event[\"event\"]\n",
    "    \n",
    "    if kind == \"on_tool_start\":\n",
    "        print(f\"ğŸ”§ Tool: {event.get('name', 'unknown')}\")\n",
    "    \n",
    "    if kind == \"on_chat_model_stream\":\n",
    "        content = event[\"data\"][\"chunk\"].content\n",
    "        if content:\n",
    "            print(content, end=\"\", flush=True)\n",
    "            token_count += 1\n",
    "            time.sleep(0.05)  # Simulate visible delay to show streaming\n",
    "\n",
    "elapsed = time.time() - start\n",
    "print(f\"\\n\\nğŸ’¡ Streamed {token_count} tokens over {elapsed:.1f}s\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a9729f",
   "metadata": {},
   "source": [
    "### Key Differences:\n",
    "\n",
    "**Non-Streaming (`.invoke()`):**\n",
    "- User waits in silence â³\n",
    "- Nothing appears until complete\n",
    "- All text arrives at once ğŸ’¥\n",
    "- Poor UX for slow operations\n",
    "\n",
    "**Streaming (`.astream_events()`):**\n",
    "- Immediate feedback ğŸ‘€\n",
    "- Tokens appear one by one\n",
    "- User sees progress in real-time âš¡\n",
    "- Feels faster even if same duration\n",
    "\n",
    "**In production apps:**\n",
    "- Streaming = ChatGPT-style typing effect\n",
    "- Non-streaming = Spinner, then sudden response\n",
    "- Streaming reduces perceived wait time by 30-50%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab1495b",
   "metadata": {},
   "source": [
    "## ä»Šæ—¥å­¦ä¹ æ€»ç»“ (Today's Learning Summary)\n",
    "\n",
    "### ğŸ¯ æ ¸å¿ƒæ¦‚å¿µ (Core Concepts)\n",
    "\n",
    "#### 1ï¸âƒ£ **Lesson 1: ä»é›¶æ„å»º ReAct Agent**\n",
    "- **ReAct æ¨¡å¼**: Thought â†’ Action â†’ Observation å¾ªç¯\n",
    "- **æ‰‹åŠ¨ Action è§£æ**: ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ `action_re = re.compile('^Action: (\\w+): (.*)$')` æå–åŠ¨ä½œ\n",
    "- **å·¥å…·é›†æˆ**: å®ç° `calculate()` å’Œ `average_dog_weight()` è‡ªå®šä¹‰å‡½æ•°\n",
    "- **ä¸Šä¸‹æ–‡ç®¡ç†**: å­¦ä¹ ç›‘æ§ token ä½¿ç”¨é‡ï¼Œé˜²æ­¢è¶…å‡ºä¸Šä¸‹æ–‡çª—å£\n",
    "- **å…³é”®å­¦ä¹ **: ç†è§£ Agent çš„åŸºæœ¬å·¥ä½œåŸç†ï¼Œå®Œå…¨æ‰‹åŠ¨æ§åˆ¶æµç¨‹\n",
    "\n",
    "#### 2ï¸âƒ£ **Lesson 2: LangGraph ç»„ä»¶**\n",
    "- **StateGraph**: ç”¨å›¾ç»“æ„å®šä¹‰ Agent å·¥ä½œæµ\n",
    "  - **Nodes**: `llm` (æ€è€ƒ) å’Œ `action` (æ‰§è¡Œå·¥å…·)\n",
    "  - **Edges**: æ¡ä»¶è¾¹ `exists_action()` å†³å®šæ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·\n",
    "- **AgentState**: `TypedDict` + `Annotated` + `operator.add` ç®¡ç†æ¶ˆæ¯å†å²\n",
    "- **å·¥å…·ç»‘å®š**: `model.bind_tools(tools)` è®© LLM çŸ¥é“å¯ç”¨å·¥å…·\n",
    "- **æœ¬åœ° Ollama é›†æˆ**: \n",
    "  - ä½¿ç”¨ `base_url=\"http://host.docker.internal:11434/v1\"`\n",
    "  - å‘ç° `qwen2.5-coder:14b` ä¸æ“…é•¿ tool calling\n",
    "  - åˆ‡æ¢åˆ° `qwen2.5:14b` è·å¾—æ›´å¥½æ•ˆæœ\n",
    "- **å…³é”®å­¦ä¹ **: ä»æ‰‹åŠ¨ç¼–ç åˆ°å£°æ˜å¼å›¾ç»“æ„ï¼Œå¤§å¹…ç®€åŒ–å¤æ‚å·¥ä½œæµ\n",
    "\n",
    "#### 3ï¸âƒ£ **Lesson 3: Agentic Search**\n",
    "- **å¯¹æ¯”æœç´¢æ–¹å¼**:\n",
    "  - **ä¼ ç»Ÿæœç´¢**: DuckDuckGo â†’ æ‰‹åŠ¨çˆ¬å– â†’ BeautifulSoup è§£æ â†’ å†—é•¿ç»“æœ\n",
    "  - **Agentic æœç´¢**: Tavily API â†’ æ™ºèƒ½ç­›é€‰ â†’ ç»“æ„åŒ–è¿”å›\n",
    "- **Tavily vs DuckDuckGo**:\n",
    "  - Tavily: ä»˜è´¹ï¼Œè´¨é‡é«˜ï¼Œè¿”å›ç²¾ç‚¼å†…å®¹\n",
    "  - DuckDuckGo: å…è´¹ï¼Œä½†ç»“æœæ··ä¹±ï¼Œéœ€è¦é¢å¤–å¤„ç†\n",
    "- **å…³é”®å­¦ä¹ **: å¥½çš„æœç´¢å·¥å…·å¯¹ Agent æ€§èƒ½è‡³å…³é‡è¦\n",
    "\n",
    "#### 4ï¸âƒ£ **Lesson 4: æŒä¹…åŒ–ä¸æµå¼è¾“å‡º**\n",
    "- **Persistence (æŒä¹…åŒ–)**:\n",
    "  - `SqliteSaver` ä¿å­˜å¯¹è¯çŠ¶æ€\n",
    "  - `thread_id` åŒºåˆ†ä¸åŒç”¨æˆ·ä¼šè¯\n",
    "  - æ”¯æŒå¤šè½®å¯¹è¯è®°å¿†ï¼ˆSF â†’ LA â†’ \"Which is warmer?\"ï¼‰\n",
    "  - ç”¨é€”: å®¢æœæœºå™¨äººã€é•¿ä»»åŠ¡æ–­ç‚¹ç»­ä¼ ã€äººå·¥å®¡æ ¸æµç¨‹\n",
    "  \n",
    "- **Streaming (æµå¼è¾“å‡º)**:\n",
    "  - `.invoke()`: åŒæ­¥ï¼Œç­‰å¾…å®Œæˆåä¸€æ¬¡æ€§è¿”å› âŒ\n",
    "  - `.astream_events()`: å¼‚æ­¥ï¼Œé€ token è¿”å› âœ…\n",
    "  - `AsyncSqliteSaver` ç”¨äºå¼‚æ­¥åœºæ™¯\n",
    "  - ç”¨é€”: ChatGPT å¼æ‰“å­—æ•ˆæœï¼Œå®æ—¶è¿›åº¦æ˜¾ç¤ºï¼Œé™ä½æ„ŸçŸ¥ç­‰å¾…æ—¶é—´ 30-50%\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ› ï¸ æŠ€æœ¯æ ˆæ€»ç»“\n",
    "\n",
    "**ç¯å¢ƒé…ç½®**:\n",
    "- Docker + Jupyter + VS Code Remote Kernel\n",
    "- æœ¬åœ° Ollama (M5 24GB RAM)\n",
    "- `.env` + `.gitignore` ä¿æŠ¤ API å¯†é’¥\n",
    "\n",
    "**å…³é”®åº“**:\n",
    "- `langgraph`: StateGraph æ„å»ºå·¥ä½œæµ\n",
    "- `langchain`: ChatOpenAI, æ¶ˆæ¯ç±»å‹, å·¥å…·å°è£…\n",
    "- `tavily`: ç½‘ç»œæœç´¢ API\n",
    "- `sqlite3`: æŒä¹…åŒ–å­˜å‚¨\n",
    "- `aiosqlite`: å¼‚æ­¥æ•°æ®åº“æ“ä½œ\n",
    "\n",
    "**æ¨¡å‹é€‰æ‹©**:\n",
    "- âŒ `qwen2.5-coder:14b`: ä»£ç ä¼˜ç§€ï¼Œä½† tool calling å·®\n",
    "- âœ… `qwen2.5:14b`: é€šç”¨æ¨¡å‹ï¼Œtool calling æ•ˆæœå¥½\n",
    "- ğŸ† `gpt-4o`: æœ€å¼ºæ€§èƒ½ï¼Œå®Œæ•´å®Œæˆå¤šæ­¥æ¨ç†\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ’¡ å®è·µç»éªŒ\n",
    "\n",
    "1. **API å¯†é’¥å®‰å…¨**: \n",
    "   - ä½¿ç”¨ `.env` æ–‡ä»¶\n",
    "   - æ·»åŠ åˆ° `.gitignore`\n",
    "   - Docker éœ€è¦å°† `.env` å¤åˆ¶åˆ°æŒ‚è½½ç›®å½•\n",
    "\n",
    "2. **æ¨¡å‹é€‰å‹**:\n",
    "   - ä»£ç æ¨¡å‹ï¼ˆcoder variantï¼‰ä¸é€‚åˆ Agent å·¥ä½œæµ\n",
    "   - æœ¬åœ°æ¨¡å‹åœ¨å¤šæ­¥æ¨ç†æ—¶å®¹æ˜“ä¸­é€”åœæ­¢\n",
    "   - GPT-4o èƒ½å®Œæ•´å®Œæˆå¤æ‚çš„å¤šæ­¥ä»»åŠ¡\n",
    "\n",
    "3. **è¾“å‡ºä¼˜åŒ–**:\n",
    "   - åˆ›å»º `print_result()` ç¾åŒ–è¾“å‡º\n",
    "   - æ·»åŠ  truncation å¤„ç†é•¿æ–‡æœ¬ï¼ˆmax_len=500ï¼‰\n",
    "   - ä½¿ç”¨ emoji æå‡å¯è¯»æ€§\n",
    "\n",
    "4. **æ€§èƒ½å¯¹æ¯”**:\n",
    "   - Tavily æœç´¢è´¨é‡ >> DuckDuckGo\n",
    "   - Streaming ç”¨æˆ·ä½“éªŒ >> Non-streaming\n",
    "   - æœ¬åœ°æ¨¡å‹é€Ÿåº¦å¿«ï¼Œä½†å‡†ç¡®æ€§ä¸å¦‚ GPT-4o\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸš€ ä¸‹ä¸€æ­¥å»ºè®®\n",
    "\n",
    "1. **ç»ƒä¹  LangGraph é«˜çº§åŠŸèƒ½**:\n",
    "   - æ›´å¤æ‚çš„æ¡ä»¶è¾¹\n",
    "   - å­å›¾ (subgraphs)\n",
    "   - å¾ªç¯ä¸ä¸­æ–­ç‚¹\n",
    "\n",
    "2. **ä¼˜åŒ–æœ¬åœ°æ¨¡å‹**:\n",
    "   - å°è¯• `mistral-small` æˆ– `llama3.1:70b`ï¼ˆå¦‚æœå†…å­˜è¶³å¤Ÿï¼‰\n",
    "   - è°ƒæ•´ system prompt æå‡ tool calling æˆåŠŸç‡\n",
    "   - å®éªŒæ¸©åº¦å’Œ top_p å‚æ•°\n",
    "\n",
    "3. **å®æˆ˜é¡¹ç›®**:\n",
    "   - æ„å»ºä¸ªäººç ”ç©¶åŠ©æ‰‹ï¼ˆpersistence + streamingï¼‰\n",
    "   - å¤šè½®å¯¹è¯å®¢æœæœºå™¨äºº\n",
    "   - ä»£ç ç”Ÿæˆä¸æ‰§è¡Œ Agent\n",
    "\n",
    "4. **æ€§èƒ½ç›‘æ§**:\n",
    "   - æ·»åŠ  token ä½¿ç”¨ç»Ÿè®¡\n",
    "   - è®°å½•å·¥å…·è°ƒç”¨æˆåŠŸç‡\n",
    "   - æµ‹é‡å“åº”æ—¶é—´ä¸å‡†ç¡®æ€§\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Š ä»Šæ—¥æˆæœ\n",
    "\n",
    "âœ… å®Œæˆ 4 ä¸ªå®Œæ•´è¯¾ç¨‹  \n",
    "âœ… æŒæ¡ ReAct æ¨¡å¼ä¸ LangGraph æ¶æ„  \n",
    "âœ… æˆåŠŸé›†æˆæœ¬åœ° Ollama æ¨¡å‹  \n",
    "âœ… ç†è§£ Persistence ä¸ Streaming çš„å®é™…åº”ç”¨  \n",
    "âœ… å»ºç«‹å®Œæ•´çš„ Agent å¼€å‘ç¯å¢ƒ  \n",
    "âœ… å­¦ä¼šè°ƒè¯•å’Œä¼˜åŒ– Agent è¾“å‡º  \n",
    "\n",
    "**æ€»å­¦ä¹ æ—¶é•¿**: çº¦ 4-6 å°æ—¶  \n",
    "**ä»£ç è¡Œæ•°**: ~500+ lines  \n",
    "**è¿è¡Œæµ‹è¯•**: 20+ æ¬¡ Agent è°ƒç”¨"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
